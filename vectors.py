#imports
import re
import numpy as np
from gensim.models import Word2Vec as w2v

#set operators that are used in c/c++ code:
ops = { 
    '(', ')', '[', ']', '.', 
    '+', '-', '*', '&', '/', 
    '%', '<', '>', '^', '|', 
    '=', ',', '?', ':' , ';',
    '{', '}', '->', '++', '--', 
    '!~', '<<', '>>', '<=', '>=', 
    '==', '!=', '&&', '||', '+=', 
    '-=', '*=', '/=', '%=', '&=',
    '^=', '|=', '<<=', '>>='
    }

class VectoriseGadget: #define class
    
    def __init__(self, vector_length):
        self.gadgets = []
        self.vector_length = vector_length
        
    def word2vec_model(self):
        model = w2v(self.gadgets, vector_size=100, window=5, min_count=1, workers=4)
        self.model = model.wv
        vocab_size = len(model.wv.key_to_index)
        print("Vocabulary size:", vocab_size)
        del model
        del self.gadgets
       
    def append_gadget(self,gadget):
        self.gadgets.append(gadget)

    def print_gadget(self):
        print(self.gadgets)

    def token_line(self,line):
        tokens = []
        non_ops = []
        tokened_line = []
        char = 0 #index number of current character
        while char < len(line):
            if line[char:char+3] in ops:
                #if char is a 3char operator
                #checks char and its proceeding 2 characters
                tokens.append(''.join(non_ops)) #append non_ops to tokens with each value seperated by ''
                tokens.append(line[char:char+3]) #append operator to tokens
                non_ops = [] #clear non_ops list
                char+=3 #increment char by 3
            elif line[char:char+2] in ops:
                #if char is a 2char operator
                #checks char and its proceeding character 
                tokens.append(''.join(non_ops))
                tokens.append(line[char:char+2])
                non_ops = []
                char+=2 #increment char by 2
            elif line[char] in ops or line[char] == ' ':
                #if char is a 1char operator or a space
                tokens.append(''.join(non_ops))
                tokens.append(line[char])
                non_ops = []
                char+=1 #increment char by 1
            else:
                #if char is a different character
                non_ops+=line[char]
                char+=1 #increment char by 1
        for i in tokens:
            if i != '' and i != ' ':
                tokened_line.append(i)
        return tokened_line
            
            
    def token_gadget(self, gadget):
        #tokenise a gadet line by line
        gadget_tokens = []
        bslice = False
        f=0
        b=0
        #print(gadget)
        function_regex = re.compile('fun(\d)+')
        for line in gadget:
            line_tokens = self.token_line(line)
            gadget_tokens+=line_tokens #adds line tokens to gadget tokens
            fb = list(filter(function_regex.match, line_tokens))
            if len(fb) > 0:
                b+=1
                bslice = True
            else:
                f+=1
        return gadget_tokens, bslice
    
    def vectorize(self, gadget):
        tokenized_gadget, backwards_slice = self.token_gadget(gadget)
        vectors = np.zeros(shape=(100, self.vector_length))
        if backwards_slice:
            for i in range(min(len(tokenized_gadget), 50)):
                vectors[100 - 1 - i] = self.model[tokenized_gadget[len(tokenized_gadget) - 1 - i]]
        else:
            for i in range(min(len(tokenized_gadget), 100)):
                vectors[i] = self.model[tokenized_gadget[i]]
        return vectors
    #vectors.flattern()
        